
==================================================
FILE: .gitignore
==================================================

camera_streamer/videos/*
!camera_streamer/videos/


==================================================
FILE: docker-compose.yml
==================================================

version: "3.8"

services:

  redis:
    image: redis:latest
    container_name: redis_server
    ports:
      - "6379:6379"
    networks:
      - safety_net
    restart: always

  postgres:
    image: postgres:15
    container_name: postgres_server
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: frames_db
    volumes:
      # persistent database
      - D:/personalproject/AP_VA_Server/postgres_data:/var/lib/postgresql/data
      # auto-init SQL files
      - D:/personalproject/AP_VA_Server/initdb:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    networks:
      - safety_net
    restart: always

    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d frames_db"]
      interval: 5s
      timeout: 5s
      retries: 10


  camera_streamer:
    build: ./camera_streamer
    image: camera_streamer
    container_name: camera_streamer_container
    environment:
      REDIS_HOST: redis_server
    volumes:
      - D:/personalproject/AP_VA_Server/camera_streamer/videos:/videos
    networks:
      - safety_net
    restart: always
    depends_on:
      - redis

  central_server:
    build: ./central_server
    image: central_server
    container_name: central_server_container
    environment:
      PG_HOST: postgres_server
      PG_USER: postgres
      PG_PASS: postgres
      PG_DB: frames_db

      REDIS_HOST: redis_server
      SHARED_DIR: /shared
    volumes:
      - D:/personalproject/AP_VA_Server/shared:/shared
    networks:
      - safety_net
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started


  # model_worker:
  #   build: ./model_worker
  #   container_name: model_worker_container
  #   image: model_worker
  #   environment:
  #     PG_HOST: postgres_server
  #     PG_PORT: "5432"
  #     PG_DB: frames_db
  #     PG_USER: postgres
  #     PG_PASSWORD: postgres

  #     REDIS_HOST: redis_server
  #     SHARED_DIR: /shared
  #   volumes:
  #     - D:/personalproject/AP_VA_Server/shared:/shared
  #   networks:
  #     - safety_net
  #   restart: always
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #     central_server:
  #       condition: service_started
  #     redis:
  #       condition: service_started

networks:
  safety_net:
    driver: bridge

==================================================
FILE: camera_streamer\app.py
==================================================

import cv2
import redis
import base64
import time
import os
import threading
from datetime import datetime

# Redis connection
redis_host = os.getenv("REDIS_HOST", "redis_server")
r = redis.StrictRedis(host=redis_host, port=6379, db=0)

# List of cameras with metadata
CAMERAS = [
    {
        "url": "videos/sample1.mp4",
        "interval": 0.5,
        "plant_id": "plantA",
        "site_id": "site1",
        "camera_code": "CAM01"
    },
    {
        "url": "videos/sample2.mkv",
        "interval": 0.5,
        "plant_id": "plantA",
        "site_id": "site2",
        "camera_code": "CAM02"
    },
    # Add more cameras here
]

def encode_frame(frame):
    """Convert frame to base64 bytes for Redis."""
    _, buffer = cv2.imencode(".jpg", frame)
    return base64.b64encode(buffer)

def camera_worker(cam_cfg):
    """Thread to read camera frames and push to Redis."""
    cam_id = cam_cfg["camera_code"]  # for logging
    print(f"üé• Camera thread started: {cam_id}")

    cap = cv2.VideoCapture(cam_cfg["url"])
    if not cap.isOpened():
        print(f"‚ùå Cannot open {cam_id}")
        return

    while True:
        ok, frame = cap.read()
        if not ok:
            print(f"‚ö†Ô∏è {cam_id}: stream ended or cannot read... restarting...")
            cap.release()
            time.sleep(2)
            cap = cv2.VideoCapture(cam_cfg["url"])
            continue

        encoded = encode_frame(frame)

        # Add current timestamp in ISO format
        timestamp = datetime.utcnow().isoformat()  # UTC time

        # Push frame + metadata to Redis
        r.xadd("camera_stream", {
            "plant_id": cam_cfg["plant_id"],
            "site_id": cam_cfg["site_id"],
            "camera_code": cam_cfg["camera_code"],
            "timestamp": timestamp,
            "frame": encoded
        })

        print(f"üì§ Sent frame: {cam_cfg['plant_id']}-{cam_cfg['site_id']}-{cam_cfg['camera_code']}-time{timestamp}")
        time.sleep(cam_cfg["interval"])

print("üöÄ Camera streamer started")

# Start a thread for each camera
threads = []
for cam_cfg in CAMERAS:
    t = threading.Thread(
        target=camera_worker,
        args=(cam_cfg,),
        daemon=True
    )
    t.start()
    threads.append(t)

# Keep main thread alive
while True:
    time.sleep(1)


==================================================
FILE: camera_streamer\Dockerfile
==================================================

FROM python:3.10-slim

WORKDIR /app

# Install OpenCV dependencies
RUN apt-get update && apt-get install -y \
    libgl1 \
    libglib2.0-0 \
    libsm6 \
    libxrender1 \
    libxext6 \
    ffmpeg

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]


==================================================
FILE: camera_streamer\requirements.txt
==================================================

numpy<2
opencv-python-headless==4.9.0.80
redis==5.0.1


==================================================
FILE: camera_streamer\videos\.gitkeep
==================================================



==================================================
FILE: central_server\central_consumer.py
==================================================

import redis
import base64
import os
import time
import json
import threading
from datetime import datetime
import psycopg2  
from psycopg2.extras import execute_values 

# -------------------------------------------------------
# 1. Redis Connection and Postgres Connection
# -------------------------------------------------------
REDIS_HOST = os.getenv("REDIS_HOST", "redis_server")
r = redis.StrictRedis(host=REDIS_HOST, port=6379, db=0)


PG_HOST = os.getenv("PG_HOST", "postgres_server")
PG_USER = os.getenv("PG_USER", "postgres")
PG_PASS = os.getenv("PG_PASS", "postgres")
PG_DB   = os.getenv("PG_DB",   "frames_db")

pg_conn = psycopg2.connect(
    host=PG_HOST,
    user=PG_USER,
    password=PG_PASS,
    dbname=PG_DB
)
pg_conn.autocommit = True
pg_cur = pg_conn.cursor()

SHARED_DIR = os.getenv("SHARED_DIR", "/shared")  # mount point inside containers
SAVE_DIR = os.path.join(SHARED_DIR, "frames")    # /shared/frames
os.makedirs(SAVE_DIR, exist_ok=True)

print("üöÄ Central consumer with batching started")


# -------------------------------------------------------
# 2. Load Config Files Safely
# -------------------------------------------------------
def load_json(path):
    try:
        with open(path, "r") as f:
            return json.load(f)
    except:
        print(f"‚ö†Ô∏è Could not read {path}, using empty")
        return {}

MODELS_CONFIG_PATH = os.path.join(SHARED_DIR, "configs", "models_config.json")
ROUTING_CONFIG_PATH = os.path.join(SHARED_DIR, "configs", "routing_config.json")

config_models = load_json(MODELS_CONFIG_PATH)
config_routing = load_json(ROUTING_CONFIG_PATH)

# Track modification timestamps
models_mtime = os.path.getmtime(MODELS_CONFIG_PATH) if os.path.exists(MODELS_CONFIG_PATH) else 0
routing_mtime = os.path.getmtime(ROUTING_CONFIG_PATH) if os.path.exists(ROUTING_CONFIG_PATH) else 0


def hot_reload_configs():
    """Reload JSON files when they change on disk."""
    global config_models, config_routing, models_mtime, routing_mtime

    while True:
        try:
            if os.path.exists(MODELS_CONFIG_PATH):
                new_m_mtime = os.path.getmtime(MODELS_CONFIG_PATH)
                if new_m_mtime != models_mtime:
                    config_models = load_json(MODELS_CONFIG_PATH)
                    models_mtime = new_m_mtime
                    print(f"üîÑ Reloaded {MODELS_CONFIG_PATH}")

            if os.path.exists(ROUTING_CONFIG_PATH):
                new_r_mtime = os.path.getmtime(ROUTING_CONFIG_PATH)
                if new_r_mtime != routing_mtime:
                    config_routing = load_json(ROUTING_CONFIG_PATH)
                    routing_mtime = new_r_mtime
                    print(f"üîÑ Reloaded {ROUTING_CONFIG_PATH}")

        except Exception as e:
            print("‚ö†Ô∏è Hot reload error:", e)

        time.sleep(1)


threading.Thread(target=hot_reload_configs, daemon=True).start()


# -------------------------------------------------------
# 3. Batch Structures
# -------------------------------------------------------
batches = {}              # {"model_name": [...]}
batch_lock = threading.Lock()
batch_start_time = {}     # {"model_name": timestamp}


# -------------------------------------------------------
# 4. Helpers
# -------------------------------------------------------
def get_model_for_camera(plant, site, camera):
    """Return model assigned for a camera."""
    try:
        return config_routing[plant][site][camera]['model']
    except:
        return None


def dispatch_batch(model_name):
    """Push completed batch to Redis stream."""
    with batch_lock:
        batch = batches.get(model_name, [])
        if not batch:
            return

        payload = json.dumps(batch)
        stream_name = f"model_queue:{model_name}"

        r.xadd(stream_name, {"batch": payload})
        print(f"üì§ Sent batch ‚Üí {model_name} | size={len(batch)}")

        batches[model_name] = []
        batch_start_time[model_name] = time.time()


def add_to_batch(model_name, item):
    with batch_lock:
        if model_name not in batches:
            batches[model_name] = []
            batch_start_time[model_name] = time.time()

        batches[model_name].append(item)


def batch_monitor():
    """Periodically dispatch timed-out batches."""
    while True:
        with batch_lock:
            timed_out = []
            for model_name, batch_items in batches.items():
                if not batch_items:
                    continue

                max_wait = config_models.get(model_name, {}).get("max_wait_time", 2)
                started = batch_start_time.get(model_name, time.time())

                if time.time() - started >= max_wait:
                    timed_out.append(model_name)


        for model_name in timed_out:
            print(f"‚è± Timeout ‚Üí dispatching {model_name}")
            dispatch_batch(model_name)

        time.sleep(5)


threading.Thread(target=batch_monitor, daemon=True).start()


# -------------------------------------------------------
# 5. MAIN LOOP
# -------------------------------------------------------
last_id = "$"   # Read ONLY new messages (important!)

while True:
    try:
        messages = r.xread({"camera_stream": last_id}, block=5000, count=1)
        if not messages:
            continue

        _, entries = messages[0]
        msg_id, fields = entries[0]
        last_id = msg_id

        # Decode metadata
        plant = fields[b"plant_id"].decode()
        site = fields[b"site_id"].decode()
        camera = fields[b"camera_code"].decode()
        timestamp = fields[b"timestamp"].decode()

        # Determine model
        model = get_model_for_camera(plant, site, camera)
        if model is None:
            print(f"‚ö†Ô∏è No model for {plant}/{site}/{camera}")
            continue

        # Convert timestamp ‚Üí folder name
        ts = datetime.fromisoformat(timestamp)
        folder_time = ts.strftime("%Y_%m_%d_%H")

        # Save decoded image
        frame_bytes = base64.b64decode(fields[b"frame"])

        save_path = os.path.join(SAVE_DIR, plant, site, camera, folder_time)
        os.makedirs(save_path, exist_ok=True)

        file_path = os.path.join(save_path, f"{msg_id.decode()}.jpg")

        with open(file_path, "wb") as f:
            f.write(frame_bytes)

        print(f"‚úî Saved {file_path}")

        # -------------------------------------------------------
        # üü¢ ADD: Insert into Postgres BEFORE batching
        # -------------------------------------------------------
        pg_cur.execute(
            """
            INSERT INTO frame_repository (frame_id, plant, site, camera, timestamp, file_path)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (frame_id) DO NOTHING
            """,
            (msg_id.decode(), plant, site, camera, timestamp, file_path)
        )
        # -------------------------------------------------------

        # DO NOT DELETE STREAM MESSAGE ‚Äî EVER.
        # If you want cleaning, use XTRIM at system level.

        # Add to batch
        add_to_batch(model, {
            "frame_path": file_path,
            "plant": plant,
            "site": site,
            "camera": camera,
            "timestamp": timestamp
        })

        # Batch full? dispatch immediately
        current_batch = batches.get(model, [])
        batch_size = config_models.get(model, {}).get("batch_size", 4)

        if len(current_batch) >= batch_size:
            print(f"üì¶ Max batch size ‚Üí {model}")
            dispatch_batch(model)

    except Exception as e:
        print("‚ùå ERROR:", e)
        time.sleep(1)


==================================================
FILE: central_server\Dockerfile
==================================================

    FROM python:3.10-slim

    WORKDIR /app

    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    COPY central_consumer.py .
    RUN mkdir central_server

    CMD ["python", "central_consumer.py"]


==================================================
FILE: central_server\requirements.txt
==================================================

numpy
redis
fastapi
uvicorn
opencv-python-headless
psycopg2-binary



==================================================
FILE: initdb\init.sql
==================================================

CREATE TABLE IF NOT EXISTS frame_repository (
    id SERIAL PRIMARY KEY,
    frame_id TEXT,
    plant TEXT,
    site TEXT,
    camera TEXT,
    timestamp TEXT,
    file_path TEXT,
    version INT DEFAULT 1
);




==================================================
FILE: model_worker\Dockerfile
==================================================

FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY model_worker.py .

CMD ["python", "model_worker.py"]


==================================================
FILE: model_worker\model_worker.py
==================================================

import os
import time
import json
import redis
import base64
import traceback

from ultralytics import YOLO
import psycopg2
from psycopg2.extras import Json

# ---------------------------------------------------------
# REDIS CONNECTION
# ---------------------------------------------------------
REDIS_HOST = os.getenv("REDIS_HOST", "redis_server")
r = redis.StrictRedis(host=REDIS_HOST, port=6379, db=0)

print("üöÄ Model Worker Started (YOLO Only)")

# ---------------------------------------------------------
# GPU CHECK
# ---------------------------------------------------------
USE_GPU = False
try:
    import torch
    if torch.cuda.is_available():
        USE_GPU = True
        print("üí† GPU detected! Using CUDA.")
    else:
        print("‚¨õ CPU mode")
except Exception:
    print("‚ùå PyTorch missing ‚Äî CPU mode only.")


# ---------------------------------------------------------
# LOAD YOLO MODEL
# ---------------------------------------------------------
device = 0 if USE_GPU else "cpu"
print(f"üì¶ Loading YOLO model on {device}...")
model = YOLO("yolov8n.pt")
model.to(device)
print("‚ú® YOLO loaded.")


# ---------------------------------------------------------
# POSTGRES CONNECTION
# ---------------------------------------------------------
pg_conn = psycopg2.connect(
    host=os.getenv("PG_HOST", "localhost"),
    port=os.getenv("PG_PORT", "5432"),
    dbname=os.getenv("PG_DB", "yolo_db"),
    user=os.getenv("PG_USER", "postgres"),
    password=os.getenv("PG_PASSWORD", "postgres")
)
pg_conn.autocommit = True
pg_cursor = pg_conn.cursor()
print("üêò Connected to PostgreSQL")


# ---------------------------------------------------------
# YOLO Inference
# ---------------------------------------------------------
def run_inference_yolo(image_paths):

    # send list of paths ‚Äî Ultralytics handles batching internally
    results_raw = model(image_paths)

    results = []
    for i, res in enumerate(results_raw):
        detections = []
        for box in res.boxes:
            detections.append({
                "cls": int(box.cls[0]),
                "conf": float(box.conf[0]),
                "xyxy": box.xyxy[0].tolist()
            })

        results.append({
            "image_path": image_paths[i],
            "detections": detections
        })

    return results


# ---------------------------------------------------------
# WORKER LOOP
# ---------------------------------------------------------
def process_batches():
    last_id = "0-0"

    while True:
        try:
            # Read batch from Redis stream
            messages = r.xread({"model_queue:yolo_model": last_id}, block=5000, count=1)
            if not messages:
                continue

            _, entries = messages[0]
            msg_id, fields = entries[0]
            last_id = msg_id

            batch_json = fields[b"batch"].decode()
            batch = json.loads(batch_json)

            image_paths = [item["frame_path"] for item in batch]

            print(f"üì• Batch received: {len(image_paths)} images")

            # Run YOLO
            results = run_inference_yolo(image_paths)

            # Save to PostgreSQL
            for item in results:
                pg_cursor.execute("""
                    INSERT INTO yolo_results (image_path, detections)
                    VALUES (%s, %s)
                """, (item["image_path"], Json(item["detections"])))

            print(f"üíæ Saved {len(results)} results")

            # Delete message from Redis
            r.xdel("model_queue:yolo_model", msg_id)

        except Exception as e:
            print("‚ùå Worker error:", e)
            traceback.print_exc()
            time.sleep(1)


process_batches()


==================================================
FILE: model_worker\requirements.txt
==================================================

redis
pillow
numpy
ultralytics
tritonclient[http]
torch  # optional for CPU inference
psycopg2-binary


==================================================
FILE: shared\configs\models_config.json
==================================================

{
  "yolo_v8": { "batch_size": 4, "max_wait_time": 2 },
  "helmet_detector": { "batch_size": 8, "max_wait_time": 3 }
}


==================================================
FILE: shared\configs\routing_config.json
==================================================

{
  "plantA": {
    "site1": {
      "CAM01": { "model" :"yolo_v8"},
      "CAM02": { "model" :"helmet_detector"}
    },
    "site2": {
      "CAM02": { "model" :"helmet_detector"}
    }
  },

  "plant9": {
    "siteZ": {
      "cam77": { "model" :"yolo_v8"}
    }
  }
}


[Content skipped: shared/frames]