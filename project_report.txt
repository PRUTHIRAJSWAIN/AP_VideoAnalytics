
==================================================
FILE: .gitignore
==================================================

camera_streamer/videos/*
!camera_streamer/videos/
configs/frames/*
!configs/frames/
project_report.txt

==================================================
FILE: docker-compose.yml
==================================================

services:

  redis:
    image: redis:latest
    container_name: redis_server
    ports:
      - "6379:6379"
    networks:
      - safety_net
    restart: always

  postgres:
    image: postgres:15
    container_name: postgres_server
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: frames_db
    volumes:
      # persistent database
      - D:/personalproject/AP_VA_Server/postgres_data:/var/lib/postgresql/data
      # auto-init SQL files
      - D:/personalproject/AP_VA_Server/initdb:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    networks:
      - safety_net
    restart: always

    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d frames_db"]
      interval: 5s
      timeout: 5s
      retries: 10


  camera_streamer:
    build: ./camera_streamer
    image: camera_streamer
    container_name: camera_streamer_container
    environment:
      REDIS_HOST: redis_server
    volumes:
      - D:/personalproject/AP_VA_Server/camera_streamer/videos:/videos
    networks:
      - safety_net
    restart: always
    depends_on:
      - redis

  central_server:
    build: ./central_server
    image: central_server
    container_name: central_server_container
    environment:
      PG_HOST: postgres_server
      PG_USER: postgres
      PG_PASS: postgres
      PG_DB: frames_db

      REDIS_HOST: redis_server
      SHARED_DIR: /shared
    volumes:
      - D:/personalproject/AP_VA_Server/shared:/shared
    networks:
      - safety_net
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started


  model_worker:
    build: ./model_worker
    container_name: model_worker_container
    image: model_worker
    environment:
      PG_HOST: postgres_server
      PG_PORT: "5432"
      PG_DB: frames_db
      PG_USER: postgres
      PG_PASSWORD: postgres

      REDIS_HOST: redis_server
      SHARED_DIR: /shared
    volumes:
      - D:/personalproject/AP_VA_Server/shared:/shared
    networks:
      - safety_net
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      central_server:
        condition: service_started
      redis:
        condition: service_started

networks:
  safety_net:
    driver: bridge


==================================================
FILE: ThingsToRunForDB
==================================================

AP_VA_SERVER
â”‚
â”œâ”€â”€ camera_streamer/        â† Sends frames â†’ Redis
â”‚   â”œâ”€â”€ videos/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ central_server/         â† Saves frames + routes batches â†’ model workers
â”‚   â”œâ”€â”€ central_consumer.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ model_worker/           â† Runs inference (YOLO / others)
â”‚   â”œâ”€â”€ model_worker.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ initdb/                 â† PostgreSQL auto-create DB tables
â”‚   â””â”€â”€ init.sql
â”‚
â”œâ”€â”€ postgres_data/          â† Volume for Postgres (auto created)
â”‚
â”œâ”€â”€ shared/                 â† Shared volume across all containers
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚    â”œâ”€â”€ models_config.json
â”‚   â”‚    â””â”€â”€ routing_config.json
â”‚   â””â”€â”€ frames/
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ docker-compose.yml


1. Multi-model sequential processing
The image goes through a chain of models where each model uses the previous modelâ€™s output, such as detecting a person, then estimating pose, then checking if the person is holding a phone.
2. Same frame processed at different frequencies
One frame can be sent to multiple models, but each model receives it based on its required frequencyâ€”fast tasks get every frame, while slower tasks like helmet detection get frames only at longer intervals.

==================================================
FILE: camera_streamer\app.py
==================================================

import cv2
import redis
import base64
import time
import os
import threading
from datetime import datetime

# Redis connection
redis_host = os.getenv("REDIS_HOST", "redis_server")
r = redis.StrictRedis(host=redis_host, port=6379, db=0)

# List of cameras with metadata
CAMERAS = [
    {
        "url": "videos/sample1.mp4",
        "interval": 0.5,
        "plant_id": "plantA",
        "site_id": "site1",
        "camera_code": "CAM01"
    },
    {
        "url": "videos/sample2.mkv",
        "interval": 0.5,
        "plant_id": "plantA",
        "site_id": "site2",
        "camera_code": "CAM02"
    },
    # Add more cameras here
]

def encode_frame(frame):
    """Convert frame to base64 bytes for Redis."""
    _, buffer = cv2.imencode(".jpg", frame)
    return base64.b64encode(buffer)

def camera_worker(cam_cfg):
    """Thread to read camera frames and push to Redis."""
    cam_id = cam_cfg["camera_code"]  # for logging
    print(f"ğŸ¥ Camera thread started: {cam_id}")

    cap = cv2.VideoCapture(cam_cfg["url"])
    if not cap.isOpened():
        print(f"âŒ Cannot open {cam_id}")
        return

    while True:
        ok, frame = cap.read()
        if not ok:
            print(f"âš ï¸ {cam_id}: stream ended or cannot read... restarting...")
            cap.release()
            time.sleep(2)
            cap = cv2.VideoCapture(cam_cfg["url"])
            continue

        encoded = encode_frame(frame)

        # Add current timestamp in ISO format
        timestamp = datetime.utcnow().isoformat()  # UTC time

        # Push frame + metadata to Redis
        r.xadd("camera_stream", {
            "plant_id": cam_cfg["plant_id"],
            "site_id": cam_cfg["site_id"],
            "camera_code": cam_cfg["camera_code"],
            "timestamp": timestamp,
            "frame": encoded
        })

        print(f"ğŸ“¤ Sent frame: {cam_cfg['plant_id']}-{cam_cfg['site_id']}-{cam_cfg['camera_code']}-time{timestamp}")
        time.sleep(cam_cfg["interval"])

print("ğŸš€ Camera streamer started")

# Start a thread for each camera
threads = []
for cam_cfg in CAMERAS:
    t = threading.Thread(
        target=camera_worker,
        args=(cam_cfg,),
        daemon=True
    )
    t.start()
    threads.append(t)

# Keep main thread alive
while True:
    time.sleep(1)


==================================================
FILE: camera_streamer\Dockerfile
==================================================

FROM python:3.10-slim

WORKDIR /app

# Install OpenCV dependencies
RUN apt-get update && apt-get install -y \
    libgl1 \
    libglib2.0-0 \
    libsm6 \
    libxrender1 \
    libxext6 \
    ffmpeg

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]


==================================================
FILE: camera_streamer\requirements.txt
==================================================

numpy<2
opencv-python-headless==4.9.0.80
redis==5.0.1


==================================================
FILE: camera_streamer\videos\.gitkeep
==================================================



==================================================
FILE: central_server\central_consumer.py
==================================================

import redis
import base64
import os
import time
import json
import threading
from datetime import datetime
import psycopg2  
from psycopg2.extras import execute_values 

# -------------------------------------------------------
# 1. Redis Connection and Postgres Connection
# -------------------------------------------------------
REDIS_HOST = os.getenv("REDIS_HOST", "redis_server")
r = redis.StrictRedis(host=REDIS_HOST, port=6379, db=0)


PG_HOST = os.getenv("PG_HOST", "postgres_server")
PG_USER = os.getenv("PG_USER", "postgres")
PG_PASS = os.getenv("PG_PASS", "postgres")
PG_DB   = os.getenv("PG_DB",   "frames_db")

pg_conn = psycopg2.connect(
    host=PG_HOST,
    user=PG_USER,
    password=PG_PASS,
    dbname=PG_DB
)
pg_conn.autocommit = True
pg_cur = pg_conn.cursor()

SHARED_DIR = os.getenv("SHARED_DIR", "/shared")  # mount point inside containers
SAVE_DIR = os.path.join(SHARED_DIR, "frames")    # /shared/frames
os.makedirs(SAVE_DIR, exist_ok=True)

print("ğŸš€ Central consumer with batching started")


# -------------------------------------------------------
# 2. Load Config Files Safely
# -------------------------------------------------------
def load_json(path):
    try:
        with open(path, "r") as f:
            return json.load(f)
    except:
        print(f"âš ï¸ Could not read {path}, using empty")
        return {}

MODELS_CONFIG_PATH = os.path.join(SHARED_DIR, "configs", "models_config.json")
ROUTING_CONFIG_PATH = os.path.join(SHARED_DIR, "configs", "routing_config.json")

config_models = load_json(MODELS_CONFIG_PATH)
config_routing = load_json(ROUTING_CONFIG_PATH)

# Track modification timestamps
models_mtime = os.path.getmtime(MODELS_CONFIG_PATH) if os.path.exists(MODELS_CONFIG_PATH) else 0
routing_mtime = os.path.getmtime(ROUTING_CONFIG_PATH) if os.path.exists(ROUTING_CONFIG_PATH) else 0


def hot_reload_configs():
    """Reload JSON files when they change on disk."""
    global config_models, config_routing, models_mtime, routing_mtime

    while True:
        try:
            if os.path.exists(MODELS_CONFIG_PATH):
                new_m_mtime = os.path.getmtime(MODELS_CONFIG_PATH)
                if new_m_mtime != models_mtime:
                    config_models = load_json(MODELS_CONFIG_PATH)
                    models_mtime = new_m_mtime
                    print(f"ğŸ”„ Reloaded {MODELS_CONFIG_PATH}")

            if os.path.exists(ROUTING_CONFIG_PATH):
                new_r_mtime = os.path.getmtime(ROUTING_CONFIG_PATH)
                if new_r_mtime != routing_mtime:
                    config_routing = load_json(ROUTING_CONFIG_PATH)
                    routing_mtime = new_r_mtime
                    print(f"ğŸ”„ Reloaded {ROUTING_CONFIG_PATH}")

        except Exception as e:
            print("âš ï¸ Hot reload error:", e)

        time.sleep(1)


threading.Thread(target=hot_reload_configs, daemon=True).start()


# -------------------------------------------------------
# 3. Batch Structures
# -------------------------------------------------------
batches = {}              # {"model_name": [...]}
batch_lock = threading.Lock()
batch_start_time = {}     # {"model_name": timestamp}


# -------------------------------------------------------
# 4. Helpers
# -------------------------------------------------------
def get_model_for_camera(plant, site, camera):
    """Return model assigned for a camera."""
    try:
        return config_routing[plant][site][camera]['model']
    except:
        return None


def dispatch_batch(model_name):
    """Push completed batch to Redis stream."""
    with batch_lock:
        batch = batches.get(model_name, [])
        if not batch:
            return

        payload = json.dumps(batch)
        stream_name = f"model_queue:{model_name}"

        r.xadd(stream_name, {"batch": payload})
        print(f"ğŸ“¤ Sent batch â†’ {model_name} | size={len(batch)}")

        batches[model_name] = []
        batch_start_time[model_name] = time.time()


def add_to_batch(model_name, item):
    with batch_lock:
        if model_name not in batches:
            batches[model_name] = []
            batch_start_time[model_name] = time.time()

        batches[model_name].append(item)


def batch_monitor():
    """Periodically dispatch timed-out batches."""
    while True:
        with batch_lock:
            timed_out = []
            for model_name, batch_items in batches.items():
                if not batch_items:
                    continue

                max_wait = config_models.get(model_name, {}).get("max_wait_time", 2)
                started = batch_start_time.get(model_name, time.time())

                if time.time() - started >= max_wait:
                    timed_out.append(model_name)


        for model_name in timed_out:
            print(f"â± Timeout â†’ dispatching {model_name}")
            dispatch_batch(model_name)

        time.sleep(5)


threading.Thread(target=batch_monitor, daemon=True).start()


# -------------------------------------------------------
# 5. MAIN LOOP
# -------------------------------------------------------
last_id = "$"   # Read ONLY new messages (important!)

while True:
    try:
        messages = r.xread({"camera_stream": last_id}, block=5000, count=1)
        if not messages:
            continue

        _, entries = messages[0]
        msg_id, fields = entries[0]
        last_id = msg_id

        # Decode metadata
        plant = fields[b"plant_id"].decode()
        site = fields[b"site_id"].decode()
        camera = fields[b"camera_code"].decode()
        timestamp = fields[b"timestamp"].decode()

        # Determine model
        model = get_model_for_camera(plant, site, camera)
        if model is None:
            print(f"âš ï¸ No model for {plant}/{site}/{camera}")
            continue

        # Convert timestamp â†’ folder name
        ts = datetime.fromisoformat(timestamp)
        folder_time = ts.strftime("%Y_%m_%d_%H")

        # Save decoded image
        frame_bytes = base64.b64decode(fields[b"frame"])

        save_path = os.path.join(SAVE_DIR, plant, site, camera, folder_time)
        os.makedirs(save_path, exist_ok=True)

        file_path = os.path.join(save_path, f"{msg_id.decode()}.jpg")

        with open(file_path, "wb") as f:
            f.write(frame_bytes)

        print(f"âœ” Saved {file_path}")

        # -------------------------------------------------------
        # ğŸŸ¢ ADD: Insert into Postgres BEFORE batching
        # -------------------------------------------------------
        pg_cur.execute(
            """
            INSERT INTO frame_repository (frame_id, plant, site, camera, timestamp, file_path)
            VALUES (%s, %s, %s, %s, %s, %s)
            RETURNING id
            """,
            (msg_id.decode(), plant, site, camera, timestamp, file_path)
        )
        db_id = pg_cur.fetchone()[0]
        # -------------------------------------------------------

        # DO NOT DELETE STREAM MESSAGE â€” EVER.
        # If you want cleaning, use XTRIM at system level.

        # Add to batch
        add_to_batch(model, {
            "frame_path": file_path,
            "plant": plant,
            "site": site,
            "camera": camera,
            "timestamp": timestamp,
            "frame_db_id": db_id
        })

        # Batch full? dispatch immediately
        current_batch = batches.get(model, [])
        batch_size = config_models.get(model, {}).get("batch_size", 4)

        if len(current_batch) >= batch_size:
            print(f"ğŸ“¦ Max batch size â†’ {model}")
            dispatch_batch(model)

    except Exception as e:
        print("âŒ ERROR:", e)
        time.sleep(1)


==================================================
FILE: central_server\Dockerfile
==================================================

    FROM python:3.10-slim

    WORKDIR /app

    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    COPY central_consumer.py .
    RUN mkdir central_server

    CMD ["python", "central_consumer.py"]


==================================================
FILE: central_server\requirements.txt
==================================================

numpy
redis
fastapi
uvicorn
opencv-python-headless
psycopg2-binary



==================================================
FILE: initdb\init.sql
==================================================

CREATE TABLE IF NOT EXISTS frame_repository (
    id SERIAL PRIMARY KEY,
    frame_id TEXT,
    plant TEXT,
    site TEXT,
    camera TEXT,
    timestamp TEXT,
    file_path TEXT,
    version INT DEFAULT 1
);
CREATE TABLE IF NOT EXISTS model_detections (
    frame_id INT REFERENCES frame_repository(id),
    model_id INT,
    detection JSONB,
    timestamp TIMESTAMP DEFAULT NOW(),
    PRIMARY KEY(frame_id, model_id)
);


==================================================
FILE: model_worker\Dockerfile
==================================================

FROM python:3.10

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY model_worker.py .

CMD ["python", "model_worker.py"]


==================================================
FILE: model_worker\model_worker.py
==================================================

import os
import time
import json
import redis
import base64
import traceback
import threading
from abc import ABC, abstractmethod

# Optional imports for specific backends
try:
    from ultralytics import YOLO
    import torch
except ImportError:
    YOLO = None
    torch = None

import psycopg2
from psycopg2.extras import Json



# ---------------------------------------------------------
# GLOBAL CONFIG
# ---------------------------------------------------------
REDIS_HOST = os.getenv("REDIS_HOST", "redis_server")
PG_HOST = os.getenv("PG_HOST", "postgres_server")
PG_PORT = os.getenv("PG_PORT", "5432")
PG_DB = os.getenv("PG_DB", "frames_db")
PG_USER = os.getenv("PG_USER", "postgres")
PG_PASSWORD = os.getenv("PG_PASSWORD", "postgres")

SHARED_DIR = os.getenv("SHARED_DIR", "/shared")
MODELS_DIR = os.path.join(SHARED_DIR, "models")
MODELS_CONFIG_PATH = os.path.join(SHARED_DIR, "configs", "models_config.json")

# ---------------------------------------------------------
# DATABASE & REDIS HELPERS
# ---------------------------------------------------------
def get_pg_connection():
    conn = psycopg2.connect(
        host=PG_HOST,
        port=PG_PORT,
        dbname=PG_DB,
        user=PG_USER,
        password=PG_PASSWORD
    )
    conn.autocommit = True
    return conn

def get_redis_connection():
    return redis.StrictRedis(host=REDIS_HOST, port=6379, db=0)

# ---------------------------------------------------------
# ABSTRACT BACKEND
# ---------------------------------------------------------
class ModelBackend(ABC):
    def __init__(self, model_name, config):
        self.model_name = model_name
        self.config = config
        self.device = "cpu"
        
    @abstractmethod
    def load(self):
        """Load the model into memory. Raise Exception if file missing."""
        pass

    @abstractmethod
    def infer(self, image_paths):
        """Run inference on a list of image paths. Returns list of dicts."""
        pass

# ---------------------------------------------------------
# YOLO BACKEND
# ---------------------------------------------------------
class YOLOBackend(ModelBackend):
    def __init__(self, model_name, config):
        super().__init__(model_name, config)
        self.model = None
        # Always look inside /shared/models/
        weights_name = config.get("weights_file", f"{model_name}.pt")
        self.weights_file = os.path.join(MODELS_DIR, weights_name)

        # Check for GPU
        if torch and torch.cuda.is_available():
            self.device = 0
            print(f"[{self.model_name}] ğŸ’  GPU detected! Using CUDA.")
        else:
            print(f"[{self.model_name}] â¬› CPU mode")

    def load(self):
        if YOLO is None:
            raise ImportError("Ultralytics not installed")

        # Check if weights file exists locally
        if not os.path.exists(self.weights_file):
            raise FileNotFoundError(
                f"âš ï¸ Weights file missing locally (no download allowed): {self.weights_file}"
            )
        print(f"[{self.model_name}] ğŸ“¦ Loading YOLO model: {self.weights_file} on {self.device}...")
        self.model = YOLO(self.weights_file)
        self.model.to(self.device)
        print(f"[{self.model_name}] âœ¨ YOLO loaded.")

    def infer(self, image_paths):
        # Ultralytics handles batching internally
        results_raw = self.model(image_paths, verbose=False)
        
        results = []
        for i, res in enumerate(results_raw):
            detections = []
            for box in res.boxes:
                detections.append({
                    "cls": int(box.cls[0]),
                    "conf": float(box.conf[0]),
                    "xyxy": box.xyxy[0].tolist()
                })

            results.append({
                "image_path": image_paths[i],
                "detections": detections
            })
        return results

# ---------------------------------------------------------
# WORKER THREAD
# ---------------------------------------------------------
def worker_loop(backend):
    """Function to run in a separate thread for each model."""
    r = get_redis_connection()
    pg_conn = get_pg_connection()
    pg_cursor = pg_conn.cursor()
    
    stream_name = f"model_queue:{backend.model_name}"
    last_id = "0-0"
    
    print(f"[{backend.model_name}] ğŸš€ Worker thread started. Listening to {stream_name}")

    while True:
        try:
            # Read batch from Redis stream
            messages = r.xread({stream_name: last_id}, block=5000, count=1)
            if not messages:
                continue

            _, entries = messages[0]
            msg_id, fields = entries[0]
            last_id = msg_id

            batch_json = fields[b"batch"].decode()
            batch = json.loads(batch_json)

            image_paths = [item["frame_path"] for item in batch]
            print(f"[{backend.model_name}] ğŸ“¥ Batch received: {len(image_paths)} images")

            # Run Inference
            results = backend.infer(image_paths)


            # Save to PostgreSQL
            for i, item in enumerate(results):
                batch_item = batch[i]
                frame_db_id = batch_item.get("frame_db_id")
                model_id = backend.config.get("model_id")
                
                if frame_db_id is not None and model_id is not None:
                    pg_cursor.execute("""
                        INSERT INTO model_detections (frame_id, model_id, detection)
                        VALUES (%s, %s, %s)
                        ON CONFLICT (frame_id, model_id) 
                        DO UPDATE SET detection = EXCLUDED.detection, timestamp = NOW();
                    """, (frame_db_id, model_id, Json(item["detections"])))
                else:
                    # Fallback to old table or log warning
                    print(f"[{backend.model_name}] âš ï¸ custom save skipped (missing IDs). frame_id={frame_db_id} model_id={model_id}")
                    pg_cursor.execute("""
                        INSERT INTO detection_results (image_path, detections)
                        VALUES (%s, %s)
                    """, (item["image_path"], Json(item["detections"])))

            print(f"[{backend.model_name}] ğŸ’¾ Saved {len(results)} results")

            # Delete message from Redis
            r.xdel(stream_name, msg_id)

        except Exception as e:
            print(f"[{backend.model_name}] âŒ Error:", e)
            traceback.print_exc()
            time.sleep(1)

# ---------------------------------------------------------
# MAIN LOADER
# ---------------------------------------------------------
def load_config():
    try:
        with open(MODELS_CONFIG_PATH, "r") as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸ Could not read {MODELS_CONFIG_PATH}: {e}")
        return {}

def main():
    print("ğŸš€ Model Worker Manager Started")
    # Wait for Redis/DB to be ready
    time.sleep(5) 
    
    config = load_config()
    threads = []

    for model_name, model_cfg in config.items():
        print(f"ğŸ”§ Initializing backend for: {model_name}")
        
        # FACTORY LOGIC (Expand here for Triton, etc.)
        # For now, we assume everything is YOLO unless specified otherwise
        backend_type = model_cfg.get("type", "yolo")
        
        try:
            if backend_type == "yolo":
                backend = YOLOBackend(model_name, model_cfg)
            else:
                print(f"âš ï¸ Unknown backend type '{backend_type}' for {model_name}. Skipping.")
                continue               
            # Load model (this might fail if file is missing)
            backend.load()
            
            # Start Thread
            t = threading.Thread(target=worker_loop, args=(backend,), daemon=True)
            t.start()
            threads.append(t)
            
        except Exception as e:
            print(f"âš ï¸ FAILED to start worker for {model_name}: {e}")
            print(f"   -> This model will NOT process frames.")
            continue

    if not threads:
        print("âŒ No workers started. Exiting...")
        return

    # Keep main thread alive
    while True:
        time.sleep(1)

if __name__ == "__main__":
    main()


==================================================
FILE: model_worker\requirements.txt
==================================================

opencv-python-headless
redis
pillow
numpy
ultralytics
tritonclient[http]
torch
psycopg2-binary


==================================================
FILE: shared\configs\models_config.json
==================================================

{
  "yolo_v8": {
    "batch_size": 4,
    "max_wait_time": 2,
    "type": "yolo",
    "weights_file": "yolov11s.pt",
    "model_id": 1
  },
  "helmet_detector": {
    "batch_size": 8,
    "max_wait_time": 3,
    "type": "yolo",
    "weights_file": "yolov11s.pt",
    "model_id": 2
  }
}

==================================================
FILE: shared\configs\routing_config.json
==================================================

{
  "plantA": {
    "site1": {
      "CAM01": { "model" :"yolo_v8"},
      "CAM02": { "model" :"helmet_detector"}
    },
    "site2": {
      "CAM02": { "model" :"helmet_detector"}
    }
  },

  "plant9": {
    "siteZ": {
      "cam77": { "model" :"yolo_v8"}
    }
  }
}


[Content skipped: shared/frames]