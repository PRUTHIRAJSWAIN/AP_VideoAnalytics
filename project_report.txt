
camera_streamer

central_server

File name: docker-compose.yml
Content: services:

  redis:
    image: redis:latest
    container_name: redis_server
    ports:
      - "6379:6379"
    networks:
      - safety_net
    restart: always

  camera_streamer:
    image: camera_streamer
    container_name: camera_streamer_container
    environment:
      REDIS_HOST: redis_server
    networks:
      - safety_net
    restart: always
    depends_on:
      - redis

  central_server:
    image: central_server
    container_name: central_server_container
    environment:
      REDIS_HOST: redis_server
    volumes:
      - D:/personalproject/AP_VA_Server/central_server/output:/received_frames
      - D:/personalproject/AP_VA_Server/central_server:/app    # shared configs
    networks:
      - safety_net
    restart: always
    depends_on:
      - redis

  model_worker:
    build: ./model_worker          # new folder, new service
    container_name: model_worker_container
    environment:
      REDIS_HOST: redis_server
    volumes:
      # same shared folder so model_worker sees saved images
      - D:/personalproject/AP_VA_Server/central_server/output:/received_frames
    networks:
      - safety_net
    restart: always
    depends_on:
      - redis
      - central_server

networks:
  safety_net:
    driver: bridge


File name: extractor.py
Content: from pathlib import Path

def read_file(file_path):
    """Reads content of a file, returns content if it's a readable text file."""
    try:
        # Open file with 'utf-8' encoding
        return file_path.read_text(encoding='utf-8')
    except UnicodeDecodeError:
        # If the file is not a text file, return None
        return None
    except Exception as e:
        # Log other errors (like permission issues) and return None
        print(f"Error reading file {file_path}: {e}")
        return None

def generate_report(root_folder):
    """Generates a report with the folder structure and file content."""
    report = []

    # Walk through all folders and files in the root folder
    for path in root_folder.rglob('*'):  # rglob('*') recursively gets all files and directories
        # Get relative path from root_folder
        rel_path = path.relative_to(root_folder)
        
        # Add folder structure (if it's a directory, skip reading its content)
        if path.is_dir():
            report.append(f"\n{rel_path}")
        elif path.is_file():
            file_content = read_file(path)
            
            if file_content:  # If content was successfully read
                report.append(f"\nFile name: {rel_path}")
                report.append(f"Content: {file_content}")
            else:  # If it's a binary or unreadable file, just append its name
                report.append(f"\nFile name: {rel_path}")
                report.append("Content: [Binary/Unreadable file]")

    return '\n'.join(report)

def save_report(report, output_file):
    """Save the generated report to a text file."""
    try:
        output_file.write_text(report, encoding='utf-8')
        print(f"Report saved to {output_file}")
    except Exception as e:
        print(f"Error saving report: {e}")

def main():
    # Get the current working directory (the folder where the script is run from)
    root_folder = Path.cwd()  # Path to the current working directory

    # Output text file to save the report
    output_file = root_folder / 'project_report.txt'

    # Check if the root folder exists
    if not root_folder.exists():
        print(f"Error: The folder '{root_folder}' does not exist.")
        return

    # Generate the report
    report = generate_report(root_folder)

    # Save the report
    save_report(report, output_file)

if __name__ == "__main__":
    print('Starting report generation...')
    main()


model_worker

File name: project_report.txt
Content: [Binary/Unreadable file]

shared

File name: camera_streamer\app.py
Content: import cv2
import redis
import base64
import time
import os
import threading
from datetime import datetime

# Redis connection
redis_host = os.getenv("REDIS_HOST", "redis_server")
r = redis.StrictRedis(host=redis_host, port=6379, db=0)

# List of cameras with metadata
CAMERAS = [
    {
        "url": "videos/sample1.mp4",
        "interval": 0.5,
        "plant_id": "plantA",
        "site_id": "site1",
        "camera_code": "CAM01"
    },
    {
        "url": "videos/sample2.mkv",
        "interval": 0.5,
        "plant_id": "plantA",
        "site_id": "site2",
        "camera_code": "CAM02"
    },
    # Add more cameras here
]

def encode_frame(frame):
    """Convert frame to base64 bytes for Redis."""
    _, buffer = cv2.imencode(".jpg", frame)
    return base64.b64encode(buffer)

def camera_worker(cam_cfg):
    """Thread to read camera frames and push to Redis."""
    cam_id = cam_cfg["camera_code"]  # for logging
    print(f"üé• Camera thread started: {cam_id}")

    cap = cv2.VideoCapture(cam_cfg["url"])
    if not cap.isOpened():
        print(f"‚ùå Cannot open {cam_id}")
        return

    while True:
        ok, frame = cap.read()
        if not ok:
            print(f"‚ö†Ô∏è {cam_id}: stream ended or cannot read... restarting...")
            cap.release()
            time.sleep(2)
            cap = cv2.VideoCapture(cam_cfg["url"])
            continue

        encoded = encode_frame(frame)

        # Add current timestamp in ISO format
        timestamp = datetime.utcnow().isoformat()  # UTC time

        # Push frame + metadata to Redis
        r.xadd("camera_stream", {
            "plant_id": cam_cfg["plant_id"],
            "site_id": cam_cfg["site_id"],
            "camera_code": cam_cfg["camera_code"],
            "timestamp": timestamp,
            "frame": encoded
        })

        print(f"üì§ Sent frame: {cam_cfg['plant_id']}-{cam_cfg['site_id']}-{cam_cfg['camera_code']}")
        time.sleep(cam_cfg["interval"])

print("üöÄ Camera streamer started")

# Start a thread for each camera
threads = []
for cam_cfg in CAMERAS:
    t = threading.Thread(
        target=camera_worker,
        args=(cam_cfg,),
        daemon=True
    )
    t.start()
    threads.append(t)

# Keep main thread alive
while True:
    time.sleep(1)


File name: camera_streamer\Dockerfile
Content: FROM python:3.10-slim

WORKDIR /app

# Install OpenCV dependencies
RUN apt-get update && apt-get install -y \
    libgl1 \
    libglib2.0-0 \
    libsm6 \
    libxrender1 \
    libxext6 \
    ffmpeg

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]


File name: camera_streamer\requirements.txt
Content: numpy<2
opencv-python-headless==4.9.0.80
redis==5.0.1


camera_streamer\videos

File name: central_server\central_consumer.py
Content: import redis
import base64
import os
import time
import json
import threading
from datetime import datetime

# -------------------------------------------------------
# 1. Redis Connection
# -------------------------------------------------------
REDIS_HOST = os.getenv("REDIS_HOST", "redis_server")
r = redis.StrictRedis(host=REDIS_HOST, port=6379, db=0)

SHARED_DIR = os.getenv("SHARED_DIR", "/shared")   # mount point inside containers
SAVE_DIR = os.path.join(SHARED_DIR, "frames")    # /shared/frames
os.makedirs(SAVE_DIR, exist_ok=True)

print("üöÄ Central consumer with batching started")


# -------------------------------------------------------
# 2. Load config files (model settings + camera routing)
# -------------------------------------------------------
def load_json(path):
    try:
        with open(path, "r") as f:
            return json.load(f)
    except:
        print(f"‚ö†Ô∏è Could not read {path}, using empty")
        return {}
    
MODELS_CONFIG_PATH = os.path.join(SHARED_DIR, "configs", "models_configs.json")
ROUTING_CONFIG_PATH = os.path.join(SHARED_DIR, "configs", "routing_config.json")

config_models = load_json(MODELS_CONFIG_PATH)
config_routing = load_json(ROUTING_CONFIG_PATH)


# Track last modification time ‚Üí for hot reload
models_mtime = os.path.getmtime(MODELS_CONFIG_PATH)
routing_mtime = os.path.getmtime(ROUTING_CONFIG_PATH)


def hot_reload_configs():
    """Reload configuration JSON files when changed."""
    global config_models, config_routing, models_mtime, routing_mtime

    while True:
        try:
            # Check models config
            new_m_mtime = os.path.getmtime(MODELS_CONFIG_PATH)
            if new_m_mtime != models_mtime:
                config_models = load_json(MODELS_CONFIG_PATH)
                models_mtime = new_m_mtime
                print(f"üîÑ Reloaded { MODELS_CONFIG_PATH }")

            # Check routing config
            new_r_mtime = os.path.getmtime(ROUTING_CONFIG_PATH)
            if new_r_mtime != routing_mtime:
                config_routing = load_json(ROUTING_CONFIG_PATH)
                routing_mtime = new_r_mtime
                print(f"üîÑ Reloaded { ROUTING_CONFIG_PATH }")

        except Exception as e:
            print("‚ö†Ô∏è Hot reload error:", e)

        time.sleep(1)


# Start hot-reload thread
threading.Thread(target=hot_reload_configs, daemon=True).start()


# -------------------------------------------------------
# 3. Batch storage structure (in memory)
# -------------------------------------------------------
batches = {}  # {"model_name": [{"path":..., "timestamp":...}, ...]}
batch_lock = threading.Lock()  # thread safety


# Track when a batch started
batch_start_time = {}  # {"model_name": timestamp}


# -------------------------------------------------------
# 4. Helper functions
# -------------------------------------------------------

def get_model_for_camera(plant, site, camera):
    """Return the model assigned to (plant, site, camera)."""
    try:
        return config_routing[plant][site][camera]
    except:
        return None


def dispatch_batch(model_name):
    """Send completed batch to Redis and clear it."""

    with batch_lock:
        batch = batches.get(model_name, [])

        if not batch:
            return  # nothing to send

        # Redis stream for model ‚Üí model batches
        stream_name = f"model_queue:{model_name}"

        # Convert batch list to JSON
        payload = json.dumps(batch)

        # Push to Redis
        r.xadd(stream_name, {"batch": payload})

        print(f"üì§ Sent batch ‚Üí {model_name} | size={len(batch)}")

        # Clear batch
        batches[model_name] = []
        batch_start_time[model_name] = time.time()


def add_to_batch(model_name, item):
    """Add image info to model batch."""
    with batch_lock:

        if model_name not in batches:
            batches[model_name] = []
            batch_start_time[model_name] = time.time()

        batches[model_name].append(item)


def batch_monitor():
    """Dispatch batches if max_wait_time exceeded."""
    while True:
        with batch_lock:
            for model_name, batch_items in batches.items():

                if not batch_items:
                    continue

                max_wait = config_models.get(model_name, {}).get("max_wait_time", 2)
                started = batch_start_time.get(model_name, time.time())

                if time.time() - started >= max_wait:
                    print(f"‚è± Timeout reached ‚Üí dispatching {model_name}")
                    dispatch_batch(model_name)

        time.sleep(0.2)


threading.Thread(target=batch_monitor, daemon=True).start()


# -------------------------------------------------------
# 5. MAIN LOOP ‚Äì Read frames ‚Üí Save ‚Üí Route ‚Üí Batch
# -------------------------------------------------------
last_id = "0-0"

while True:
    try:
        messages = r.xread({"camera_stream": last_id}, block=5000, count=1)
        if not messages:
            continue

        _, entries = messages[0]
        msg_id, fields = entries[0]
        last_id = msg_id

        # Decode metadata
        plant = fields[b"plant_id"].decode()
        site = fields[b"site_id"].decode()
        camera = fields[b"camera_code"].decode()
        timestamp = fields[b"timestamp"].decode()

        # Which model will process this camera?
        model = get_model_for_camera(plant, site, camera)
        if model is None:
            print(f"‚ö†Ô∏è No model assigned for {plant}/{site}/{camera}")
            continue

        # Convert time to folder format
        ts = datetime.fromisoformat(timestamp)
        folder_time = ts.strftime("%Y_%m_%d_%H")

        # Save image to disk
        frame_bytes = base64.b64decode(fields[b"frame"])

        save_path = os.path.join(SAVE_DIR, plant, site, camera, folder_time)
        os.makedirs(save_path, exist_ok=True)
        file_path = os.path.join(save_path, f"{msg_id}.jpg")

        with open(file_path, "wb") as f:
            f.write(frame_bytes)

        print(f"‚úî Saved {file_path}")

        # Remove from Redis to save RAM
        r.xdel("camera_stream", msg_id)

        # Add to batch
        add_to_batch(model, {
            "frame_path": file_path,
            "plant": plant,
            "site": site,
            "camera": camera,
            "timestamp": timestamp
        })

        # Check if batch full ‚Üí send immediately
        current_batch = batches.get(model, [])
        batch_size = config_models.get(model, {}).get("batch_size", 4)

        if len(current_batch) >= batch_size:
            print(f"üì¶ Max batch size reached ‚Üí {model}")
            dispatch_batch(model)

    except Exception as e:
        print("‚ùå ERROR:", e)
        time.sleep(1)


File name: central_server\Dockerfile
Content:     FROM python:3.10-slim

    WORKDIR /app

    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    COPY central_consumer.py .
    RUN mkdir central_server

    CMD ["python", "central_consumer.py"]


central_server\output

File name: central_server\requirements.txt
Content: numpy
redis
fastapi
uvicorn
opencv-python-headless



File name: model_worker\Dockerfile
Content: FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY model_worker.py .

CMD ["python", "model_worker.py"]


File name: model_worker\model_worker.py
Content: import os
import time
import json
import redis
import base64
import traceback

from PIL import Image
import numpy as np

# ---------------------------------------------------------
# REDIS CONNECTION
# ---------------------------------------------------------
REDIS_HOST = os.getenv("REDIS_HOST", "redis_server")
r = redis.StrictRedis(host=REDIS_HOST, port=6379, db=0)

print("üöÄ Model Worker Started (Auto GPU/CPU Mode)")

# ---------------------------------------------------------
# AUTO DETECT MODE (GPU/TRITON or CPU)
# ---------------------------------------------------------

USE_GPU = False
USE_TRITON = False

try:
    import torch

    if torch.cuda.is_available():
        USE_GPU = True
        print("üí† GPU detected! CUDA available.")
    else:
        print("‚¨õ No GPU found (CUDA unavailable).")

except Exception:
    print("‚ùå PyTorch missing, cannot detect GPU.")

# Try Triton client
try:
    import tritonclient.http as httpclient
    USE_TRITON = True
    print("üì° Triton client available.")
except Exception:
    print("‚ö†Ô∏è Triton client not available; fallback to CPU YOLO.")

# ---------------------------------------------------------
# LOAD MODEL ACCORDING TO MODE
# ---------------------------------------------------------

if USE_TRITON:
    try:
        triton_client = httpclient.InferenceServerClient(url="triton_server:8000")
        print("üîó Connected to Triton server.")
    except Exception:
        print("‚ùå Triton server not reachable. Falling back to CPU.")
        USE_TRITON = False

if not USE_TRITON:
    # Load Ultralytics YOLO model
    from ultralytics import YOLO
    try:
        print("üì¶ Loading YOLO model on CPU...")
        model = YOLO("yolov8n.pt")   # put your model here
        print("‚ú® CPU YOLO loaded.")
    except Exception as e:
        print("‚ùå Error loading CPU model:", e)
        raise

# ---------------------------------------------------------
# HELPER: Run Inference
# ---------------------------------------------------------
def run_inference_cpu(image_paths):
    """Run inference using YOLO CPU."""
    results = []

    batch = [np.array(Image.open(p)) for p in image_paths]
    output = model(batch)

    for i, res in enumerate(output):
        detections = []
        for box in res.boxes:
            detections.append({
                "cls": int(box.cls[0]),
                "conf": float(box.conf[0]),
                "xyxy": box.xyxy[0].tolist()
            })

        results.append({
            "image_path": image_paths[i],
            "detections": detections
        })

    return results


def run_inference_triton(image_paths):
    """Run inference using Triton."""
    results = []

    # TODO: Depends on your Triton model input format.
    # Placeholder implementation:
    for p in image_paths:
        results.append({
            "image_path": p,
            "detections": [{"cls": 0, "conf": 0.99, "xyxy": [10, 10, 100, 100]}]
        })

    return results

# ---------------------------------------------------------
# MAIN LOOP ‚Äì Read Batches From Redis
# ---------------------------------------------------------

def process_batches():
    last_id = "0-0"

    while True:
        try:
            messages = r.xread({"model_queue:yolo_model": last_id}, block=5000, count=1)
            if not messages:
                continue

            _, entries = messages[0]
            msg_id, fields = entries[0]
            last_id = msg_id

            batch_json = fields[b"batch"].decode()
            batch = json.loads(batch_json)

            image_paths = [item["frame_path"] for item in batch]

            print(f"üì• Received batch with {len(image_paths)} images")

            # -------------------------
            # Run inference
            # -------------------------
            if USE_TRITON:
                results = run_inference_triton(image_paths)
            else:
                results = run_inference_cpu(image_paths)

            # --------------------------
            # Push results to next queue
            # --------------------------
            r.xadd("analysis_queue", {"results": json.dumps(results)})

            print(f"üì§ Sent inference results ({len(results)} images)")

            # delete processed batch
            r.xdel("model_queue:yolo_model", msg_id)

        except Exception as e:
            print("‚ùå ERROR in worker:", e)
            traceback.print_exc()
            time.sleep(1)


process_batches()


File name: model_worker\requirements.txt
Content: redis
pillow
numpy
ultralytics
tritonclient[http]
torch  # optional for CPU inference


shared\configs

shared\frames

File name: shared\configs\models_config.json
Content: {
  "yolo_v8": { "batch_size": 4, "max_wait_time": 2 },
  "helmet_detector": { "batch_size": 8, "max_wait_time": 3 }
}


File name: shared\configs\routing_config.json
Content: {
  "plant1": {
    "siteA": {
      "cam01": "yolo_v8",
      "cam02": "helmet_detector"
    }
  },

  "plant9": {
    "siteZ": {
      "cam77": "yolo_v8"
    }
  }
}


File name: camera_streamer\videos\sample1.mp4
Content: [Binary/Unreadable file]

File name: camera_streamer\videos\sample2.mkv
Content: [Binary/Unreadable file]