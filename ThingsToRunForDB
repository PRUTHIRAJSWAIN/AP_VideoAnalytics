AP_VA_SERVER
│
├── camera_streamer/        ← Sends frames → Redis
│   ├── videos/
│   ├── app.py
│   ├── Dockerfile
│   └── requirements.txt
│
├── central_server/         ← Saves frames + routes batches → model workers
│   ├── central_consumer.py
│   ├── Dockerfile
│   └── requirements.txt
│
├── model_worker/           ← Runs inference (YOLO / others)
│   ├── model_worker.py
│   ├── Dockerfile
│   └── requirements.txt
│
├── initdb/                 ← PostgreSQL auto-create DB tables
│   └── init.sql
│
├── postgres_data/          ← Volume for Postgres (auto created)
│
├── shared/                 ← Shared volume across all containers
│   ├── configs/
│   │    ├── models_config.json
│   │    └── routing_config.json
│   └── frames/
│
├── .gitignore
└── docker-compose.yml


1. Multi-model sequential processing
The image goes through a chain of models where each model uses the previous model’s output, such as detecting a person, then estimating pose, then checking if the person is holding a phone.
2. Same frame processed at different frequencies
One frame can be sent to multiple models, but each model receives it based on its required frequency—fast tasks get every frame, while slower tasks like helmet detection get frames only at longer intervals.